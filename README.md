# ğŸ§  A.D.A.M - Adaptive and Dynamic Agent Module

**Continuous Self-Training Language Model with Revolutionary Hot/Cold Vocabulary Architecture**

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![CUDA](https://img.shields.io/badge/CUDA-11.0+-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](LICENSE)

---

## ğŸš€ What is A.D.A.M?

A.D.A.M is an experimental language model featuring:

- **ğŸ”¥ Hot/Cold Vocabulary Architecture** - Unlimited RAM vocabulary with GPU-cached hot words (LRU)
- **ğŸ“– Vocabulary Pre-Training** - Scan datasets first for stable vocabulary before training
- **âš¡ Extreme GPU Optimization** - cuBLAS GEMM, fused kernels, pipelined training
- **ğŸ§© Venn Semantic System** - Multi-head semantic clustering for generalization beyond memorization
- **ğŸŒŠ Continuous Learning** - Train indefinitely on new data, vocabulary grows automatically
- **ğŸ¯ Production-Ready** - Validation, early stopping, checkpointing, hot-reload

---

## ğŸ“‹ Table of Contents

- [âœ¨ Key Features](#-key-features)
- [âš™ï¸ Installation](#ï¸-installation)
- [ğŸ¯ Quick Start](#-quick-start)
- [ğŸ“š Usage Guide](#-usage-guide)
  - [Vocabulary Pre-Training](#vocabulary-pre-training-new)
  - [Dataset Training](#dataset-training)
  - [Wikipedia Training](#wikipedia-training)
  - [Interactive Chat](#interactive-chat)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [âš¡ Performance](#-performance)
- [ğŸ›ï¸ Configuration](#ï¸-configuration)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ Development](#-development)
- [ğŸ“„ License](#-license)

---

## âœ¨ Key Features

### ğŸ”¥ Hot/Cold Vocabulary (LRU Cache Architecture)

**Philosophy: Learn everything (RAM), cache what's used (GPU)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COLD VOCAB (RAM) - Unlimited Storage   â”‚
â”‚  â€¢ ALL words ever seen (millions)       â”‚
â”‚  â€¢ Pre-initialized embeddings           â”‚
â”‚  â€¢ Persisted with checkpoints           â”‚
â”‚  â€¢ "La RAM Ã¨ abbondante!" philosophy    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ LRU Cache (top 10k words)
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   HOT VOCAB (GPU) - Fast Access         â”‚
â”‚   â€¢ 10,000 most-used words              â”‚
â”‚   â€¢ Ultra-fast training/inference       â”‚
â”‚   â€¢ Automatic eviction & preloading     â”‚
â”‚   â€¢ Synced with CUDA MAX_WORD_VOCAB     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Zero memory limits (RAM is cheap)
- âœ… Optimal GPU utilization (only hot words)
- âœ… No loss spikes from new words
- âœ… Stable training at scale

### ğŸ“– Vocabulary Pre-Training (NEW!)

Scan your dataset **before** training to build a stable vocabulary:

```bash
# Scan dataset 2 times, then train for 10 passes
adam dataset train.jsonl --vocab-passes 2 --passes 10
```

**What happens:**
1. **Scan Phase** (passes 1-2): Read all data, discover words, count frequencies (no GPU sync = fast!)
2. **Finalization**: Load top 10k frequent words â†’ HOT vocab (GPU)
3. **Training Phase** (passes 3-12): Train with stable vocabulary (zero overhead!)

**Benefits:**
- âœ… Vocabulary complete from start (no mid-training creation)
- âœ… Top words automatically in GPU (optimal cache)
- âœ… Eliminates sync overhead during training
- âœ… Prevents loss spikes from new embeddings
- âœ… ~20% faster training, better convergence

---

## âš™ï¸ Installation

### From Source (Recommended)

```bash
git clone https://github.com/krokodil-byte/ADAM.git
cd ADAM
pip install -e .
```

### Quick Run (No Installation)

```bash
python run.py [command] [args]
```

### Requirements

- **Python** â‰¥ 3.8
- **NumPy** â‰¥ 1.20.0
- **CUDA Toolkit** (optional, for GPU acceleration)
  - Tested: CUDA 11.0 - 12.x
  - Recommended: CUDA 12.x for best performance

### Verify Installation

```bash
adam init
```

Checks CUDA availability and compiles GPU kernels.

---

## ğŸ¯ Quick Start

### 1ï¸âƒ£ Initialize System

```bash
adam init
```

### 2ï¸âƒ£ Train on Your Data

```bash
# Train on text file
adam train mydata.txt -o model.ckpt --passes 5

# Train on HuggingFace dataset with vocab pre-training
adam dataset train.jsonl --vocab-passes 2 --passes 10 \
     --validation --early-stopping

# Train on Wikipedia (streaming from API)
adam wikipedia --vocab-passes 1 --max-articles 5000 \
     --language en --validation
```

### 3ï¸âƒ£ Interactive Chat

```bash
adam chat -c model.ckpt
```

### 4ï¸âƒ£ View Statistics

```bash
adam stats -c model.ckpt
```

---

## ğŸ“š Usage Guide

### Vocabulary Pre-Training (NEW!)

Build stable vocabulary before training:

```bash
# HuggingFace Dataset
adam dataset train.jsonl \
     --vocab-passes 2 \      # Scan dataset twice
     --passes 10 \           # Then train for 10 passes
     --validation \
     --early-stopping

# Plain Text Files
adam dataset ./texts/ \
     --vocab-passes 1 \
     --passes 5 \
     --extensions .txt,.md

# Wikipedia
adam wikipedia \
     --vocab-passes 1 \      # Scan 100 articles first
     --max-articles 5000 \
     --language en
```

**Output Example:**
```
======================================================================
ğŸ“– VOCABULARY SCANNING - 2 pass(es)
======================================================================

ğŸ” Vocabulary scan pass 1/2
   Scanned 100/500 samples
   Scanned 200/500 samples
   ...
   âœ“ Pass 1 complete - 8543 words discovered

ğŸ” Vocabulary scan pass 2/2
   ...
   âœ“ Pass 2 complete - 9821 words discovered

ğŸ“š Finalizing vocabulary from scan...
   Total words discovered: 9821
   Loading top 9821 words to HOT vocab (GPU)...
   âœ… Vocabulary finalized:
      - Cold vocab: 9821 words (all in RAM)
      - Hot vocab: 9821/10000 words (in GPU)
      - Top 5 words: the(1523), of(987), and(856), to(743), a(654)
======================================================================
```

### Dataset Training

#### HuggingFace Datasets (.jsonl, .parquet, .csv)

```bash
adam dataset data.jsonl \
     --input-col question \      # Input column name
     --output-col answer \        # Output column name
     --template "{input}\n\n{output}" \
     --vocab-passes 1 \
     --passes 5 \
     --validation \
     --val-split 0.1
```

#### Plain Text Files

```bash
adam dataset ./documents/ \
     --extensions .txt,.md,.py \
     --vocab-passes 1 \
     --passes 3 \
     --auto-save 100
```

**Options:**
- `--vocab-passes N` - Number of vocabulary scanning passes (default: 0)
- `--passes N` - Training passes (default: 1)
- `--validation` - Enable validation
- `--early-stopping` - Stop when validation plateaus
- `--val-split 0.1` - Validation split (10%)
- `--auto-save N` - Auto-save every N samples
- `--preset PRESET` - Config preset (see Configuration)

### Wikipedia Training

#### Streaming from API

```bash
adam wikipedia \
     --vocab-passes 1 \          # Scan first batch
     --max-articles 10000 \
     --language en \
     --batch-size 100 \          # Fetch 100 articles per batch
     --passes 2 \                # 2 training passes per batch
     --validation \
     --val-articles 20
```

**Languages supported:** `en`, `it`, `de`, `fr`, `es`, `ja`, `zh`, etc.

#### From Local Dump

```bash
adam wikipedia dump.jsonl \
     --max-articles 5000 \
     -o wiki_model.ckpt
```

**Dump formats:** `.jsonl`, `.xml`, `.txt`

### Interactive Chat

```bash
adam chat -c model.ckpt

> Hello, how are you?
[Model responds...]

> Tell me about machine learning
[Model responds...]
```

**Commands:**
- Type your message and press Enter
- `quit`, `exit`, or `q` to exit

### TUI Dashboard

Full graphical interface:

```bash
adam
# or
adam dashboard
```

**Main Menu:**
- ğŸš€ Initialize System
- ğŸ§  Train on Text
- ğŸ“š Wikipedia Training
- ğŸ“‚ Dataset Training
- ğŸ’¬ Interactive Chat
- ğŸ“Š View Statistics
- âš™ï¸ Settings
- ğŸšª Exit

**Navigation:**
- `â†‘â†“` Arrow keys
- `Enter` to select
- `Q` to go back/quit

---

## ğŸ—ï¸ Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   VectLLMBrain                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Transformer (6 layers, 768d, 12 heads)      â”‚   â”‚
â”‚  â”‚  â€¢ Self-attention with cuBLAS GEMM           â”‚   â”‚
â”‚  â”‚  â€¢ Fused FFN kernels                         â”‚   â”‚
â”‚  â”‚  â€¢ Pipelined training (3-stage overlap)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Venn Semantic System (Multi-Head)           â”‚   â”‚
â”‚  â”‚  â€¢ 12 heads Ã— 256 clusters = 3072 total      â”‚   â”‚
â”‚  â”‚  â€¢ Gaussian activation + propagation         â”‚   â”‚
â”‚  â”‚  â€¢ Dynamic cluster updates                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Hot/Cold Vocabulary (LRU)                   â”‚   â”‚
â”‚  â”‚  â€¢ Cold: Unlimited RAM storage               â”‚   â”‚
â”‚  â”‚  â€¢ Hot: 10k GPU cache (auto-managed)         â”‚   â”‚
â”‚  â”‚  â€¢ Pre-initialized embeddings                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Vocabulary Flow

```
Text Input â†’ Tokenization â†’ Word Discovery
                                  â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Vocab Scan Mode?        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          Yes â”‚ No
                              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                                       â†“
  Build vocab in RAM                   Add to COLD vocab (RAM)
  (no GPU sync)                        Initialize embedding
                                              â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ HOT vocab full?  â”‚
                                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      No â”‚ Yes
                                         â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â†“                       â†“
                        Load to GPU           Evict LRU word
                                              Load new word
                                                     â†“
                                          Training with stable vocab
```

### Training Pipeline

```
CPU Thread                GPU Stream 0           GPU Stream 1
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Batch 1: Encode text
         â†“
Batch 1: H2D transfer  â†’  Forward pass
         â†“                      â†“
Batch 2: Encode text     â†’  Backward pass
         â†“                      â†“
Batch 2: H2D transfer  â†’  Weight update    â†’  D2H loss/stats
         â†“                                       â†“
Batch 3: Encode text                       Process stats
         ...                                    ...
```

**3-stage overlap = ~95% GPU utilization**

---

## âš¡ Performance

### Memory Usage

| Component | RAM | VRAM | Notes |
|-----------|-----|------|-------|
| Model (768d, 6L) | 200 MB | ~700 MB | Base architecture |
| Cold vocab (100k words) | 300 MB | - | RAM only |
| Hot vocab (10k words) | - | 30 MB | GPU cache |
| Training batch (512 seq) | - | 2/4 GB | Activations |
| **Total (typical)** | **~1 GB** | **~3/5 GB** | Comfortable for 8GB+ GPUs |

---

## ğŸ›ï¸ Configuration

### Configuration Presets

Quick profiles for different scenarios:

```bash
# Fast experimentation (10x learning rate)
adam dataset data.jsonl --preset fast_learning

# Production training (stable, conservative)
adam dataset data.jsonl --preset stable

# Maximum GPU performance
adam dataset data.jsonl --preset high_performance

# Memory-constrained GPUs
adam dataset data.jsonl --preset memory_efficient

# Chat/inference only (no training)
adam chat -c model.ckpt --preset inference
```

| Preset | LR | Momentum | Best For |
|--------|-----|----------|----------|
| `default` | 0.0001 | 0.9 | General-purpose |
| `fast_learning` | 0.001 | 0.7 | Quick experiments |
| `stable` | 0.00001 | 0.95 | Production training |
| `research` | 0.0005 | 0.9 | Venn system research |
| `inference` | 0 | - | Generation only |
| `high_performance` | - | - | Max GPU speed |
| `memory_efficient` | - | - | Limited VRAM |
| `max_throughput` | - | - | Absolute max speed |

### Key Settings

#### Hot/Cold Vocabulary

```python
MAX_WORD_VOCAB_SIZE = 10000    # MUST match CUDA (hot vocab size)
MAX_HOT_VOCAB = 10000          # GPU cache size
WORD_CREATION_THRESHOLD = 5    # Create word after N occurrences
WORD_PRUNING_THRESHOLD = 0     # 0 = never prune (recommended)
ENABLE_TOKEN_PRELOADING = True # Auto-load tokens before forward pass
SAVE_COLD_VOCAB = True         # Persist cold vocab to disk
```

#### Training

```python
BASE_LR = 0.0001               # Base learning rate
EMBEDDING_LR_SCALE = 0.1       # Embeddings 10x slower (stable)
MOMENTUM = 0.9                 # SGD momentum
VALIDATION_SPLIT = 0.1         # 10% for validation
EARLY_STOPPING_PATIENCE = 5    # Stop after 5 validations w/o improvement
AUTO_SAVE_FREQUENCY = 1000     # Auto-save every N samples
```

#### Venn Semantic System

```python
NUM_VENN_HEADS = 12            # Number of Venn heads
VENN_CLUSTERS_PER_HEAD = 256   # Clusters per head
VENN_PROPAGATION_FACTOR = 0.2  # Activation propagation strength
VENN_INTERSECTION_THRESHOLD = 0.3  # Cluster similarity threshold
```

#### Performance

```python
USE_CUBLAS = True              # cuBLAS matrix ops
ENABLE_FUSED_KERNELS = True    # Fused attention+FFN
PIPELINE_MODE = "triple"       # 3-stage pipeline overlap
GPU_UTILIZATION_TARGET = 0.9   # Target 90% GPU usage
```

### TUI Settings Menu

Access via `adam` â†’ **âš™ï¸ Settings**:

```
âš™ï¸ Settings
â”œâ”€â”€ ğŸ—ï¸ Model Architecture
â”‚   â”œâ”€â”€ Embedding Dimension: 768
â”‚   â”œâ”€â”€ Num Heads: 12
â”‚   â”œâ”€â”€ Num Layers: 6
â”‚   â””â”€â”€ Max Sequence Length: 512
â”œâ”€â”€ ğŸ“ˆ Training Parameters
â”‚   â”œâ”€â”€ Base Learning Rate: 0.0001
â”‚   â”œâ”€â”€ Momentum: 0.9
â”‚   â”œâ”€â”€ Validation Split: 0.1
â”‚   â””â”€â”€ Early Stopping: On
â”œâ”€â”€ âš¡ Performance
â”‚   â”œâ”€â”€ cuBLAS: On
â”‚   â”œâ”€â”€ Fused Kernels: On
â”‚   â”œâ”€â”€ Pipeline: triple
â”‚   â””â”€â”€ GPU Target: 90%
â””â”€â”€ ğŸ’¾ Save Settings
```

---

## ğŸ“ Project Structure

```
ADAM/
â”œâ”€â”€ setup.py                     # Package installation
â”œâ”€â”€ run.py                       # Development runner
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ LICENSE                      # CC BY-NC 4.0
â””â”€â”€ A.D.A.M â€” Adaptive and Dynamic Agent Module/
    â”œâ”€â”€ __main__.py              # Entry point
    â”œâ”€â”€ cli/
    â”‚   â””â”€â”€ adam.py              # CLI interface
    â”œâ”€â”€ core/
    â”‚   â”œâ”€â”€ brain_wrapper.py     # Main model wrapper
    â”‚   â”œâ”€â”€ vocabulary.py        # Dynamic vocabulary (hot/cold)
    â”‚   â”œâ”€â”€ config.py            # Configuration system
    â”‚   â”œâ”€â”€ stats.py             # Statistics collector
    â”‚   â”œâ”€â”€ pipeline.py          # Pipelined trainer
    â”‚   â””â”€â”€ constants.py         # System constants
    â”œâ”€â”€ modules/
    â”‚   â”œâ”€â”€ dataset_training.py  # Dataset trainer (HF + plain text)
    â”‚   â”œâ”€â”€ wikipedia_training.py # Wikipedia trainer (API + dump)
    â”‚   â”œâ”€â”€ training_logger.py   # Training logger
    â”‚   â”œâ”€â”€ chat.py              # Interactive chat
    â”‚   â””â”€â”€ tui.py               # TUI dashboard
    â”œâ”€â”€ Utils/
    â”‚   â”œâ”€â”€ checkpoint.py        # Checkpoint management
    â”‚   â”œâ”€â”€ tokenizer.py         # Text tokenization
    â”‚   â””â”€â”€ compiler.py          # CUDA compilation
    â”œâ”€â”€ kernels/
    â”‚   â””â”€â”€ brain.cu             # CUDA kernels
    â””â”€â”€ tests/                   # Test suite
```

---

## ğŸ”§ Development

### Run Tests

```bash
pip install -e ".[dev]"
pytest
```

### Test Coverage

```bash
pytest --cov=. --cov-report=html
open htmlcov/index.html
```

### Code Style

```bash
# Format code
black .

# Type check
mypy .

# Lint
flake8 .
```

---

## ğŸ› Troubleshooting

### Word Index Out of Range Error

**Fixed in latest version!** Make sure `MAX_WORD_VOCAB_SIZE` in Python matches CUDA:

```python
# config.py
MAX_WORD_VOCAB_SIZE = 10000  # âœ… Matches CUDA

# kernels/brain.cu
#define MAX_WORD_VOCAB_SIZE 10000  // âœ… Same value
```

### CUDA Out of Memory

1. **Reduce batch size**: `--batch-size 50` (default: 100)
2. **Use memory preset**: `--preset memory_efficient`
3. **Disable pipeline**: Set `PIPELINE_MODE = "disabled"` in settings
4. **Monitor with**: `nvidia-smi -l 1`

### Slow Training

1. **Enable all optimizations**: `--preset high_performance`
2. **Use vocab pre-training**: `--vocab-passes 2`
3. **Check GPU utilization**: Should be >90%
4. **Verify cuBLAS**: Settings â†’ Performance â†’ cuBLAS: On

---

## ğŸ“ Changelog

### Latest Updates

#### ğŸ‰ NEW: Vocabulary Pre-Training (2025-01)
- **Feature**: Scan datasets before training to build stable vocabulary
- **API**: `--vocab-passes N` flag for all trainers
- **Impact**: ~20% faster training, eliminates vocab overhead
- **Details**: See [Vocabulary Pre-Training](#vocabulary-pre-training-new)

#### ğŸ› FIX: Word Index Errors (2025-01)
- **Issue**: Python `max_word_vocab_size` (100k) exceeded CUDA limit (10k)
- **Fix**: Synced Python config with CUDA `MAX_WORD_VOCAB_SIZE = 10000`
- **Impact**: Eliminates "word out of index" errors

#### ğŸ§¹ CLEANUP: Legacy Code Removal (2025-01)
- **Removed**: Legacy sync methods, sequential training fallbacks
- **Removed**: `ENABLE_VOCAB_OPTIMIZATION` flag (always enabled now)
- **Impact**: Cleaner codebase, only optimized paths

#### ğŸ”§ FIX: Preset Override Bug (2025-01)
- **Issue**: Presets completely replaced user settings
- **Fix**: Presets now MERGE with existing config (preserves customizations)
- **API**: `set_config_from_preset(name, override_user_settings=False)`

---

## ğŸ“„ License

This project is licensed under the **Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0)**.

**You are free to:**
- âœ… Share â€” copy and redistribute
- âœ… Adapt â€” remix, transform, build upon

**Under these terms:**
- ğŸ“ Attribution â€” Give appropriate credit
- ğŸš« NonCommercial â€” No commercial use

See [LICENSE](LICENSE) for full details.

---

## ğŸ‘¤ Author

**Scuglia Samuele**

---

## ğŸ”— Links

- **GitHub**: [https://github.com/krokodil-byte/ADAM](https://github.com/krokodil-byte/ADAM)
- **Issues**: [Report bugs or request features](https://github.com/krokodil-byte/ADAM/issues)
- **Discussions**: [Ask questions or share ideas](https://github.com/krokodil-byte/ADAM/discussions)

---

## ğŸ™ Acknowledgments

- CUDA community for optimization resources
- HuggingFace for dataset ecosystem
- Open-source ML community

---

**Made with â¤ï¸ for the AI research community with the help of AI**
